{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "from nltk import tree\n",
    "import re\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "with open('kwords.json', 'r') as f:\n",
    "    s = ' '.join([l for l in f])\n",
    "    kw = eval(s)\n",
    "params_kw = {kw_['name']:kw_['keywords'] for kw_ in kw if kw_['group'] == 'parameter'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_kw = {}\n",
    "for kw_ in kw:\n",
    "    if kw_['group'] == 'parameter':\n",
    "        filter_kw[kw_['name']] = {'keywords':kw_['keywords'], 'type':kw_['type']}\n",
    "        if 'categories' in kw_:\n",
    "            filter_kw[kw_['name']]['categories'] = kw_['categories']\n",
    "        else:\n",
    "            filter_kw[kw_['name']]['condition_kw'] = kw_['condition_kw']\n",
    "        if 'default' in kw_:\n",
    "            filter_kw[kw_['name']]['default'] = kw_['default']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_group(itree,pos=[]):\n",
    "    if not isinstance(itree, unicode):\n",
    "        if itree.label() == 'IN':\n",
    "            if itree[0].lower() == 'by':\n",
    "                return pos\n",
    "        else:\n",
    "            i = 0\n",
    "            for subtree in itree:\n",
    "                res = find_group(subtree,pos+[i])\n",
    "                i += 1\n",
    "                if not res is None:\n",
    "                    return res\n",
    "    return None \n",
    "\n",
    "def find_token_id(target_boundaries, tokens):\n",
    "    for t in tokens:\n",
    "        if target_boundaries[1]<=t[3] and target_boundaries[1]>t[2]:\n",
    "            return(t[1]-1)\n",
    "\n",
    "def get_all_nps(itree, l=[]):\n",
    "    if not isinstance(itree, unicode):\n",
    "        if itree.label() == 'NP':\n",
    "            if len(itree) == 1 and itree[0].label() == 'PRP':\n",
    "                return l\n",
    "            l2 = len(l)\n",
    "            for subtree in itree:\n",
    "                l = get_all_nps(subtree, l)\n",
    "            if l2 == len(l):\n",
    "                l.append(itree)\n",
    "        else:\n",
    "            if itree.label() == 'PP' and itree.height() <= 3:\n",
    "                l.append(itree)\n",
    "            else:\n",
    "                for subtree in itree:\n",
    "                    l = get_all_nps(subtree, l)\n",
    "    return l\n",
    "\n",
    "def get_first_np(itree):\n",
    "    if not isinstance(itree, unicode):\n",
    "        if itree.label() == 'NP':\n",
    "            if len(itree) == 1 and itree[0].label() == 'PRP':\n",
    "                return 0\n",
    "            for subtree in itree:\n",
    "                res = get_first_np(subtree)\n",
    "                if res:\n",
    "                    return res\n",
    "            return itree\n",
    "        else:\n",
    "            for subtree in itree:\n",
    "                res = get_first_np(subtree)\n",
    "                if res:\n",
    "                    return res\n",
    "    return 0\n",
    "\n",
    "def match_param(string):\n",
    "    for p_name, p_vals in params_kw.iteritems():\n",
    "        for p_val in p_vals:\n",
    "            if re.search(p_val,string):\n",
    "                return (p_val,p_name)\n",
    "    return ('','')\n",
    "\n",
    "def match_filter(string):\n",
    "    result = []\n",
    "    for p_name, p_vals in filter_kw.iteritems():\n",
    "        \n",
    "        for p_val in p_vals['keywords']:\n",
    "            result += [(p_name, m.start(), m.end()) for m in re.finditer(p_val, string)]\n",
    "            #if re.findall(p_val,w):\n",
    "            #    result.append(p_name)\n",
    "            #    break\n",
    "    return result\n",
    "\n",
    "def match_categories(string, cats):\n",
    "    ocats = []\n",
    "    for cat_name,cat_kw_list in cats.iteritems():\n",
    "        for kw in cat_kw_list:\n",
    "            if re.search(kw, string):\n",
    "                ocats.append(cat_name)\n",
    "    return ocats\n",
    "\n",
    "def preproc(istr):\n",
    "    ostr = re.sub(r'((?=\\D) %|% (?=\\D))', ' percentage ', istr)\n",
    "    #ostr = re.sub(r'(?i)\\b(I|me|we|us|you)\\b', '', istr)\n",
    "    return ostr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_it(text):\n",
    "    text2 = preproc(text)\n",
    "            \n",
    "    output = nlp.annotate(text2, properties={\n",
    "      'annotators': 'parse',\n",
    "      'outputFormat': 'json'\n",
    "      })\n",
    "    oresult = output['sentences'][0]\n",
    "    result = oresult['parse']\n",
    "    mytree = tree.Tree.fromstring(result)\n",
    "    parameters = {}\n",
    "    target = ('','')\n",
    "\n",
    "    first_char_id = oresult[u'tokens'][0]['characterOffsetBegin']\n",
    "    tokens = [(t[u'originalText'],\n",
    "       t[u'index'],\n",
    "       t[u'characterOffsetBegin']-first_char_id,\n",
    "       t[u'characterOffsetEnd']-first_char_id) for t in oresult[u'tokens']]\n",
    "\n",
    "    deps = oresult['collapsed-ccprocessed-dependencies']\n",
    "\n",
    "    original_text = ''\n",
    "    index_ = 0\n",
    "    for t in tokens:\n",
    "        nspaces = t[2]-index_\n",
    "        index_ = t[3]\n",
    "        original_text += ' '*nspaces + t[0]\n",
    "\n",
    "    # parsing sentence type\n",
    "    if mytree[0].label()=='SBARQ':\n",
    "        if mytree[0][0].label()=='WHNP':\n",
    "            if mytree[0][0][0].label()=='WP':\n",
    "                if mytree[0][0][0][0].lower() == 'who':\n",
    "                    parameters['mode'] = 'model'\n",
    "                    # print 'Who'\n",
    "                    # print('-- run np search for target --')\n",
    "                else:\n",
    "                    pass\n",
    "                    # print 'What1'\n",
    "                    # print('-- run np search for target --')\n",
    "\n",
    "                # question body #\n",
    "            elif mytree[0][0][0].label()=='WHNP' and mytree[0][0][0][0].label()=='WHADJP':\n",
    "                parameters['mode'] = 'query'\n",
    "                parameters['aggregation'] = 'count'\n",
    "                st = mytree[0][0][0].flatten()\n",
    "                for i in range(len(st)):\n",
    "                    target = match_param(st[-i-1])\n",
    "                    if target[0]:\n",
    "                        break\n",
    "                # print('-- run np search for target --')\n",
    "\n",
    "            elif mytree[0][0][0].label()=='WDT':\n",
    "                parameters['mode'] = 'query'\n",
    "                # print 'What2'\n",
    "                # print('-- run np search for target --')\n",
    "\n",
    "                # search target in question question #\n",
    "            else:\n",
    "                pass\n",
    "                #print('-- run np search for target -- --- no question type ---')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    elif (mytree[0].label()=='S' and mytree[0][0].label()=='VP') or mytree[0].label()=='SQ':\n",
    "        if mytree[0].label()!='SQ':\n",
    "            if mytree[0][0][0].label() == 'VB' and mytree[0][0][0] in ['list','table']:\n",
    "                parameters['view'] = ['table']\n",
    "        # print('-- run np search for target -- --- no question type ---')\n",
    "\n",
    "    elif mytree[0].label()=='NP':\n",
    "        pass\n",
    "        # print('-- run np search for target -- --- no question type ---')\n",
    "        # search target #\n",
    "    else:\n",
    "        pass\n",
    "        # print mytree\n",
    "        # search target #\n",
    "\n",
    "    # match target if it is not yet matched\n",
    "    # rule: first np - last matching nn\n",
    "    if not target[0]:\n",
    "        st = get_first_np(mytree).flatten()\n",
    "        for i in range(len(st)):\n",
    "            target = match_param(st[-i-1])\n",
    "            if target[0]:\n",
    "                break\n",
    "\n",
    "    # parse dependancies, add special cases\n",
    "    if target[0]:\n",
    "        target_boundaries = [(m.start(), m.end()) \n",
    "                                 for m in re.finditer(target[0], original_text)][0]\n",
    "        target_id = find_token_id(target_boundaries, tokens)\n",
    "\n",
    "        target_token = tokens[target_id][0]\n",
    "\n",
    "        target_deps = [d for d in deps \n",
    "                           if d[u'dependentGloss'].lower() == target_token.lower() \n",
    "                               or \n",
    "                               d[u'governorGloss'].lower() == target_token.lower()]\n",
    "\n",
    "        for dep in target_deps:\n",
    "            if dep['dep'] == u'amod' and dep[u'dependentGloss'].lower() in ['average','mean']:\n",
    "                parameters['aggregation'] = 'average'\n",
    "            if dep['dep'] == u'amod' and dep[u'dependentGloss'].lower() in ['total','summary','overall','many']:\n",
    "                parameters['aggregation'] = 'count/sum'\n",
    "\n",
    "            if dep[u'governorGloss'].lower() in ['distribution','distributed']:\n",
    "                parameters['hist'] = 'distribution'\n",
    "            if dep[u'dependentGloss'].lower() in ['distribution','distributed']:\n",
    "                parameters['hist'] = 'distribution'\n",
    "            if dep['dep'] == u'nummod':\n",
    "                parameters['limit'] = dep[u'dependentGloss']\n",
    "    if 'percent' in original_text.lower():\n",
    "        parameters['aggregation'] = 'percentage'\n",
    "\n",
    "\n",
    "    # find group-by parameter\n",
    "    group_by = []\n",
    "    address = find_group(mytree)\n",
    "    if address:\n",
    "        group_st = mytree[address[:-1]]\n",
    "        group_st = get_first_np(group_st)\n",
    "        group_st = ' '.join(group_st.flatten())\n",
    "        group_by = match_filter(group_st)\n",
    "        group_by = group_by[0][0]\n",
    "\n",
    "    # match filter parametes in original text\n",
    "    filter_by = match_filter(original_text)\n",
    "\n",
    "    filter_by = [( tmp[0], original_text[tmp[1]:tmp[2]] ) \n",
    "                     for tmp in filter_by \n",
    "                     if tmp[0] not in group_by]\n",
    "\n",
    "    filter_by = [tmp for tmp in filter_by \n",
    "                     if tmp[0] not in [target[1], 'customer']]\n",
    "\n",
    "    matched_cats = []\n",
    "    filter_by_val = {}\n",
    "    if len(filter_by):\n",
    "        tokens_vals = [t[0] for t in tokens]\n",
    "\n",
    "        for filteri in filter_by:\n",
    "            if filteri[0] not in filter_by_val:\n",
    "                filter_by_val[filteri[0]] = {'original':filteri[1]}\n",
    "                filter_by_val[filteri[0]]['cats'] = []\n",
    "\n",
    "            filteri_boundaries = [(m.start(), m.end()) for m in re.finditer(filteri[1], original_text)][0]\n",
    "            filteri_id = find_token_id(filteri_boundaries, tokens)\n",
    "            print filteri_id\n",
    "\n",
    "            # replace this cycle with search through dependancies\n",
    "            for iter_i in range(min(filteri_id,2)+1):\n",
    "                string = ' '.join( tokens_vals[ filteri_id-iter_i : filteri_id+1 ] )\n",
    "\n",
    "                if 'categories' in filter_kw[filteri[0]]:\n",
    "                    matched_cats_i = match_categories(string, filter_kw[filteri[0]]['categories'])\n",
    "                else:\n",
    "                    matched_cats_i = match_categories(string, filter_kw[filteri[0]]['condition_kw'])\n",
    "                if matched_cats_i:\n",
    "                    filter_by_val[filteri[0]]['cats'] += matched_cats_i\n",
    "            filter_by_val[filteri[0]]['cats'] = list(set(filter_by_val[filteri[0]]['cats']))\n",
    "\n",
    "    \"\"\"\n",
    "    print 'text:'\n",
    "    print original_text\n",
    "\n",
    "    print 'params:'\n",
    "    print parameters\n",
    "\n",
    "    print 'target:'\n",
    "    print target[1]\n",
    "\n",
    "    print 'filter:'\n",
    "    print filter_by_val\n",
    "\n",
    "    print 'group_by'\n",
    "    print group_by\n",
    "    \"\"\"\n",
    "    \n",
    "    sql_select = []\n",
    "    sql_filter = []\n",
    "    sql_group_by = []\n",
    "    if target[1]:\n",
    "        if group_by:\n",
    "            if filter_kw[group_by]['type'] == 'cat':\n",
    "                sql_group_by.append(group_by)\n",
    "            \n",
    "        if 'hist' in parameters:\n",
    "            sql_select.append('count(*)')\n",
    "            sql_group_by.append(target[1])\n",
    "        if filter_kw[target[1]]['type'] == 'special':\n",
    "            sql_select.append('count(*)')\n",
    "        if filter_kw[target[1]]['type'] == 'cat':\n",
    "            sql_group_by.append(target[1])\n",
    "            sql_select.append('count(*)')\n",
    "        elif filter_kw[target[1]]['type'] == 'num':\n",
    "            if 'aggregation' in parameters:\n",
    "                if parameters['aggregation'] == 'average':\n",
    "                    sql_select.append('AVG('+target[1]+')')\n",
    "                elif parameters['aggregation'] == 'count/sum':\n",
    "                    sql_select.append('SUM('+target[1]+')')\n",
    "        elif filter_kw[target[1]]['type'] == 'num':\n",
    "            # add in future\n",
    "            pass\n",
    "    \n",
    "    for f_name,values in filter_by_val.iteritems():\n",
    "        if filter_kw[f_name]['type'] == 'cat':\n",
    "            all_cats = filter_kw[f_name]['categories'].keys()\n",
    "            values = values['cats']\n",
    "            if not values:\n",
    "                if 'default' in filter_kw[f_name]:\n",
    "                    values = [filter_kw[f_name]['default']]\n",
    "            left_cats = set(all_cats).difference(values)\n",
    "            \n",
    "            if not left_cats:\n",
    "                sql_group_by.append(f_name)\n",
    "            else:\n",
    "                tmp_condition = f_name+' in (\\''+'\\',\\''.join(values)+'\\')'\n",
    "                sql_filter.append(tmp_condition) \n",
    "    \n",
    "    print 'SELECT'\n",
    "    print sql_select\n",
    "    print 'WHERE'\n",
    "    print sql_filter\n",
    "    print 'GROUP BY'\n",
    "    print sql_group_by\n",
    "            \n",
    "\n",
    "    return original_text, parameters, target[1], filter_by_val, group_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "queries = [\"Show me 10 customers who buys credit cards in New York in this year? \",\n",
    "\"How customers who use credit cards differ from others? \",\n",
    "\"What is average age of customers by state? \",\n",
    "\"Show us average age of those customers who has credit cards. \",\n",
    "\"Do you know what is average customer age? \",\n",
    "\"How many males vs females % ? \",\n",
    "\"Who is my target for selling debt funds? \",\n",
    "\"Males by state. \",\n",
    "\"Average income of our customers by age. \",\n",
    "\"How many males vs females %?\",\n",
    "\"What % have insurance?\",\n",
    "\"Who is my target for selling debt funds?\",\n",
    "\"Show me who my target for selling debt funds is.\",\n",
    "\"Show me age distribution of those customers who have credit card.\",\n",
    "\"Show me gender distribution by states.\",\n",
    "\"Show me gender distribution by age.\",\n",
    "\"Show me gender distribution by income.\",\n",
    "\"Show me gender distribution by employment status.\",\n",
    "\"What is gender distribution of my customers?\",\n",
    "\"What is gender distribution of customers with credit cards?\",\n",
    "\"Show me distribution by age among males.\",\n",
    "\"Show me distribution by age among males in Texas.\",\n",
    "\"What is average income by age?\",\n",
    "\"How many customers in Alabama has more than one child?\",\n",
    "\"How many customers in Alabama has more than two children?\",\n",
    "\"How many customers in Alabama has no children?\",\n",
    "\"Show me average number of children by state.\",\n",
    "\"Show me average income by gender.\",\n",
    "\"What average income is by employment status?\",\n",
    "\"Show me 10 customers who buys most debt funds in New York in this year?\",\n",
    "\"List customers from Alabama with largest income.\",\n",
    "\"Who should I target for selling credit cards in Florida?\",\n",
    "\"Who usually buys debit cards?\",\n",
    "\"Who usually buys debit cards among unemployed people?\",\n",
    "\"Show me employment status by gender.\",\n",
    "\"Show me employment status by age for women.\",\n",
    "\"How many percentage has family?\",\n",
    "\"Show me how many % has family by age?\",\n",
    "\"Show me how many customers have no insurance by state.\"]\n",
    "i = 0\n",
    "for q in queries:\n",
    "    print i\n",
    "    res = parse_it(q)\n",
    "    for r in res:\n",
    "        print r\n",
    "    print \"\\n\\n\"\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question taxonomy\n",
    "Our analyses can parse 6 main types of question:\n",
    "1. **SBARQ**\n",
    "> Direct question introduced by a wh-word or a wh-phrase. Indirect questions and relative clauses should be bracketed as SBAR, not SBARQ.\n",
    "\n",
    "    First subtree structures:\n",
    "    1. __WHNP - WP__  \n",
    "        Noun questions: (Who?, What?)  \n",
    "        _Who_ questions are categorized as modelling commands and data is being processed according to model specific scenario.  \n",
    "        _What_ questions will be processed further  \n",
    "        <br>\n",
    "    2. __WHNP - WHNP - WHADJP__  \n",
    "        _How many_ type of question  \n",
    "        <br>\n",
    "    3. __WHNP - WDT__  \n",
    "        _What_ type of question          \n",
    "        <br>\n",
    "\n",
    "2. **S-VP** or **SQ**\n",
    "    - Simple declarative clause, i.e. one that is not introduced by a (possible empty) subordinating conjunction or a wh-word and that does not exhibit subject-verb inversion.\n",
    "    - VP - Vereb Phrase.  \n",
    "    - Inverted yes/no question, or main clause of a wh-question, following the wh-phrase in SBARQ.\n",
    "    Direct commands, like: _show, display, list_\n",
    "3. **NP**\n",
    "    - Noun pharase  \n",
    "    Simple statement\n",
    "\n",
    "4. **others**\n",
    "    For all other forms there is a set of parsing rules\n",
    "\n",
    "_http://web.mit.edu/6.863/www/PennTreebankTags.html_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
