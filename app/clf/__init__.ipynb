{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import psycopg2\n",
    "import psycopg2.extensions\n",
    "\n",
    "from py2neo import Graph\n",
    "from twitter import Twitter, OAuth\n",
    "\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import random\n",
    "import csv\n",
    "from string import strip\n",
    "\n",
    "# for sending offers via gmail\n",
    "import base64\n",
    "from email.mime.image import MIMEImage\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import smtplib\n",
    "    \n",
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.customer_database\n",
    "\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "graph = Graph(config[\"neo4j\"])\n",
    "\n",
    "if 'http_proxy' in os.environ:\n",
    "    if 'https_proxy' not in os.environ:\n",
    "        os.environ[\"https_proxy\"] = os.environ[\"http_proxy\"]\n",
    "\n",
    "twitter = Twitter(auth=OAuth(config['token'], config['token_secret'], config['consumer_key'], config['consumer_secret']))\n",
    "\n",
    "field_names =  ['user_screen_name', 'user_name', 'id_str', 'created_at', \n",
    "                'sentiment', 'categories', 'text', 'klout_score', 'segment_id',\n",
    "               'got_reply', 'followers_count', 'is_agent_reply']\n",
    "\n",
    "segment_dict = {0:'-', 1:'Mass Market', 2:'Young Professional',\n",
    "                3:'Mass Afluent', 4:'Affluent', 5:'High Net Worth'}\n",
    "\n",
    "suboffers = {} # offers dictionary\n",
    "with open('offers_suboffers.csv', 'rb') as csvfile:\n",
    "    file_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in file_reader:\n",
    "        if len(row) >= 3:          # check if we have at least 3 field\n",
    "            key = strip(row[0])\n",
    "            value = strip(row[1])\n",
    "            imageId = strip(row[2])\n",
    "            text = strip(row[3]) if len(row) >= 4 else \"\"\n",
    "            if key in suboffers:\n",
    "                suboffers[key].append({\"name\": value, \"image\": imageId, \"text\":text})\n",
    "            else:\n",
    "                suboffers[key] = [{\"name\": value, \"image\": imageId, \"text\":text}]\n",
    "\n",
    "def get_priority_key(tw_l):\n",
    "    for tw in tw_l:\n",
    "        if not tw['is_agent_reply']:\n",
    "            p = (tw['segment_id']/2 - tw['sentiment'] - tw['got_reply']*5 +\n",
    "                tw['followers_count']/1000 + int('fraud' in tw['categories']))\n",
    "            return -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, jsonify, request, send_file\n",
    "from flask_sockets import Sockets\n",
    "from json import dumps\n",
    "import gevent\n",
    "import time\n",
    "import StringIO\n",
    "\n",
    "app = Flask(__name__)\n",
    "sockets = Sockets(app)\n",
    "\n",
    "\"\"\"\n",
    "@sockets.route('/twitter_stream')\n",
    "def twitter_stream(ws):\n",
    "    j = 0\n",
    "    \n",
    "    conn = psycopg2.connect(database=\"twitter\", user=\"postgres\")\n",
    "    conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    curs = conn.cursor()\n",
    "    curs.execute(\"LISTEN new_tweet;\")\n",
    "    \n",
    "    while not ws.closed:\n",
    "        conn.poll()\n",
    "        t0 = time.time()\n",
    "        if conn.notifies:\n",
    "            notify = conn.notifies.pop(0)\n",
    "            \n",
    "            ws.send(\"{notification:\\'new tweet\\'}\")\n",
    "            j+=1\n",
    "        if time.time() - t0 > 3600:\n",
    "            print(\"1hr timeout\")\n",
    "            break\n",
    "        else:\n",
    "            gevent.sleep()\n",
    "    conn.close()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@app.route('/reply', methods=['POST'])\n",
    "def profile():\n",
    "    data = eval(request.form['py_data'])\n",
    "    status = request.form['status']\n",
    "    status_id = request.form['status_id']\n",
    "    \n",
    "    sentiment = data['sentiment']\n",
    "    user_name = data['user_name']\n",
    "    \n",
    "    tweet = twitter.statuses.update(status=status, in_reply_to_status_id=status_id)\n",
    "    \n",
    "    conn = psycopg2.connect(database=config[\"twitter_db\"][\"database\"], user=config[\"twitter_db\"][\"user\"])\n",
    "    conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    curs = conn.cursor()\n",
    "    \n",
    "    curs.execute(\"\"\"update customers\n",
    "                set got_reply=1\n",
    "                where user_screen_name=%s\"\"\",(tweet['in_reply_to_screen_name'],))\n",
    "    \n",
    "    curs.execute(\"\"\"insert into stream (id_str, text, created_at, in_reply_to_status_id,\n",
    "                in_reply_to_screen_name, user_screen_name, user_name, sentiment, is_agent_reply)\n",
    "                values (%s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\",\n",
    "                 (tweet['id_str'],\n",
    "                  tweet['text'],\n",
    "                  tweet['created_at'],\n",
    "                  tweet['in_reply_to_status_id'],\n",
    "                  tweet['in_reply_to_screen_name'],\n",
    "                  tweet['in_reply_to_screen_name'],\n",
    "                  user_name,\n",
    "                  sentiment,\n",
    "                  1,))\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    return 'good'\n",
    "\n",
    "@app.route('/send_offer', methods=['POST'])\n",
    "def send_offer():\n",
    "    recipient_name = request.form['recipient-name']\n",
    "    message_text = request.form['message-text']\n",
    "    offer_image_id = request.form['offer-image-id']\n",
    "\n",
    "    gmail_user = config[\"gmail_user\"]\n",
    "    gmail_pass = config[\"gmail_pass\"]\n",
    "    gmail_recipient = config[\"gmail_recipient\"]\n",
    "    TO = gmail_recipient if type(gmail_recipient) is list else [gmail_recipient]\n",
    "\n",
    "    msg = MIMEMultipart('alternative')\n",
    "    msg['Subject'] = \"Special offer from CapBank\"\n",
    "    msg['From'] = \"VIP clients support team\"\n",
    "    msg['To'] = recipient_name\n",
    "    \n",
    "    fp = open(\"static/images/email/email_tmp\"+offer_image_id+\".png\", 'rb')\n",
    "    img_b64 = base64.b64encode(fp.read()).decode('ascii')\n",
    "    fp.close()\n",
    "\n",
    "    html = \"\"\"<html>\n",
    "      <head></head>\n",
    "      <body>\n",
    "        <img src=\"data:image/png;base64,{0}\" /><br />\n",
    "        <pre>{1}</pre>\n",
    "      </body>\n",
    "    </html>\"\"\".format(img_b64, message_text)\n",
    "    \n",
    "    # Record the MIME types of both parts - text/plain and text/html.\n",
    "    part1 = MIMEText(text, 'plain')\n",
    "    part2 = MIMEText(html, 'html')    \n",
    "    \n",
    "    # Attach parts into message container.\n",
    "    # According to RFC 2046, the last part of a multipart message, in this case\n",
    "    # the HTML message, is best and preferred.\n",
    "    msg.attach(part1)\n",
    "    msg.attach(part2)\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "        server.ehlo()\n",
    "        server.starttls()\n",
    "        server.login(gmail_user, gmail_pass)\n",
    "        server.sendmail(gmail_user, gmail_recipient, msg.as_string())\n",
    "        server.close()\n",
    "        res = '<h4 class=\"bg-success\">E-mail sent successfully</h4>'\n",
    "    except:\n",
    "        res = '<h4 class=\"bg-danger\">E-mail sending failed</h4>'\n",
    "\n",
    "    return res\n",
    "\n",
    "@app.route('/data', methods=['GET'])\n",
    "def provide_data():\n",
    "    conn = psycopg2.connect(database=\"twitter\", user=\"postgres\")\n",
    "    conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    curs.execute(\"\"\"select stream.user_screen_name, stream.user_name, stream.id_str,\n",
    "                stream.created_at, stream.sentiment, stream.categories, stream.text,\n",
    "                customers.klout_score, customers.segment, customers.got_reply, \n",
    "                customers.followers_count, stream.is_agent_reply\n",
    "                from stream\n",
    "                INNER JOIN customers\n",
    "                ON customers.user_screen_name=stream.user_screen_name\n",
    "                where stream.user_screen_name in \n",
    "                (\n",
    "                    select tmp.user_screen_name\n",
    "                    from (\n",
    "                        select user_screen_name as user_screen_name, max(id_str) as max_id_str\n",
    "                        from stream\n",
    "                        group by user_screen_name\n",
    "                    ) tmp\n",
    "                    left join (\n",
    "                        select user_screen_name, got_reply\n",
    "                        from customers\n",
    "                    ) tmp2\n",
    "                    on tmp.user_screen_name=tmp2.user_screen_name\n",
    "                    order by tmp2.got_reply asc, tmp.max_id_str desc limit 10\n",
    "                )\n",
    "                order by customers.got_reply desc, stream.id_str desc\n",
    "                ;\"\"\")\n",
    "    rec = curs.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    user_blocks = {v[0]:[] for v in rec}\n",
    "    for row in rec:\n",
    "        if len(user_blocks[row[0]]) < 10:\n",
    "            user_blocks[row[0]] += [{k:v for k,v in zip(field_names, row)}]\n",
    "            user_blocks[row[0]][-1]['id_str'] = str(user_blocks[row[0]][-1]['id_str'])\n",
    "            user_blocks[row[0]][-1]['segment'] = segment_dict[user_blocks[row[0]][-1]['segment_id']]\n",
    "    \n",
    "    user_blocks_v = user_blocks.values()\n",
    "    user_blocks_v_s = sorted(user_blocks_v, key=get_priority_key)\n",
    "    \n",
    "    return jsonify({'data':user_blocks_v_s})\n",
    "\n",
    "@app.route('/graph')\n",
    "def get_graph():\n",
    "    results = graph.cypher.execute(\n",
    "        \"match (p)-[r]->(p1) \"\n",
    "        \"where ((r.favorited>0 and r.retweeted>0) or (r.favorited>0 and r.replied>0) or (r.retweeted>0 and r.replied>0) \"\n",
    "        \"or (r.favorited>1) or (r.retweeted>1) or (r.replied>1)) and (not p=p1) \"\n",
    "        \"return p.screen_name, sum(r.favorited)+sum(r.retweeted)+sum(r.replied) as cardinality, p1.screen_name \"\n",
    "        \"limit 500;\")\n",
    "    nodes = []\n",
    "    rels = []\n",
    "\n",
    "    persons = {}\n",
    "    for person_b, cardinality, person_a in results:\n",
    "        if person_a in persons:\n",
    "            persons[person_a] += 1\n",
    "        else:\n",
    "            persons[person_a] = 1\n",
    "\n",
    "        if person_b not in persons:\n",
    "            persons[person_b] = 1\n",
    "\n",
    "    for i, v in persons.iteritems():\n",
    "        if i == u'gunjan_amit':\n",
    "            person = {\n",
    "                \"id\": persons.keys().index(i),\n",
    "                \"caption\": i, \n",
    "                \"role\": \"center\"}\n",
    "        else:\n",
    "            person = {\n",
    "                \"id\": persons.keys().index(i),\n",
    "                \"caption\": i, \n",
    "                \"role\": \"customer\" if np.random.rand()>0.9 else \"user\",\n",
    "                }\n",
    "        nodes.append(person)\n",
    "\n",
    "    for person_b, cardinality, person_a in results:\n",
    "        person = {\"caption\": person_a, \"role\": \"customer\"}\n",
    "        source = persons.keys().index(person_a)\n",
    "        target = persons.keys().index(person_b)\n",
    "        rels.append({\"source\": source, \n",
    "            \"target\": target, \n",
    "            \"weight\": cardinality,\n",
    "            \"caption\": \"Value: {0}\".format(cardinality)\n",
    "            })\n",
    "\n",
    "    return jsonify({\"comment\": \"Tweeter users graph\", \"nodes\": nodes, \"edges\": rels})\n",
    "\n",
    "@app.route('/download/customers')\n",
    "def index():\n",
    "    strIO = StringIO.StringIO()\n",
    "    strIO.write('Customer ID, NBO\\n')\n",
    "    for i in range(10):\n",
    "        strIO.write(str(np.floor(10000*np.random.rand()))+',Credit Card\\n')\n",
    "    \n",
    "    strIO.seek(0)\n",
    "    return send_file(strIO,\n",
    "                     attachment_filename=\"Customers.csv\",\n",
    "                     as_attachment=True)\n",
    "\n",
    "@app.route('/map/get_customers/<state>')\n",
    "def map_get_customers(state):\n",
    "    try:\n",
    "        #cursor = db.customers.find({ 'state' : state })\n",
    "        cursor = db.customers.find(\n",
    "            { 'state' : state },\n",
    "            {'_id':  0,'customerId':1,'name':1,'churn_rate':1,'status':1}\n",
    "        )\n",
    "\n",
    "        status_weights = {'platinum':10,'gold':9,'silver':8,'bronze':7}\n",
    "\n",
    "        customers = []\n",
    "        for c in cursor:\n",
    "            c.update({'importance':status_weights[c['status'].lower()]*c['churn_rate']})\n",
    "            customers.append(c)\n",
    "\n",
    "        customers = sorted(customers, key=lambda x:-x['importance'])[:100]\n",
    "\n",
    "        customers = [[c['importance'],\n",
    "                      c['customerId'],\n",
    "                      c['name'],\n",
    "                      str(int(c['churn_rate']*100))+' %',\n",
    "                      c['status'],] \n",
    "                     for c in customers]\n",
    "        # Set selected user\n",
    "        customers[0][1] = 1\n",
    "        customers[0][2] = 'Kathleen Fanning'\n",
    "        return jsonify({'data':customers})\n",
    "    \n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/map/get_states_data')\n",
    "def map_states_data():\n",
    "    try:\n",
    "        with open('static/js/us-states.json') as f:\n",
    "            us_states_obj = json.load(f)\n",
    "\n",
    "        states_data = db.customers.aggregate([\n",
    "            {\"$group\": {\"_id\": \"$state\", \n",
    "                        'averageChurn': { '$avg': \"$churn_rate\" },\n",
    "                        \"count\": {\"$sum\": 1}}}])\n",
    "        states_data = {row['_id']:row for row in states_data}\n",
    "\n",
    "        states_data_objs = []\n",
    "\n",
    "        acc = 0\n",
    "        for i in range(len(us_states_obj['features'])):\n",
    "            state_name = us_states_obj['features'][i][u'properties']['name']\n",
    "            if state_name in states_data:\n",
    "                us_states_obj['features'][i]['properties'].update(states_data[state_name])\n",
    "                states_data_objs.append(us_states_obj['features'][i])\n",
    "\n",
    "        us_states_obj['features'] = states_data_objs\n",
    "\n",
    "        return jsonify(us_states_obj)\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/map')\n",
    "def mappage():\n",
    "    try:\n",
    "        return render_template(\"map.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/cluster')\n",
    "def clusterpage():\n",
    "    try:\n",
    "        return render_template(\"cluster.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/nbo/<int:userid>')\n",
    "def nbopage(userid):\n",
    "    try:\n",
    "        cursor = db.customers.find({ 'customerId' : userid })\n",
    "        if cursor.count() < 1:\n",
    "            return render_template(\"nbo_no_such_user.html\")\n",
    "        mongo_record = cursor.next()\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "    customer = {}\n",
    "    # General fields\n",
    "    data_list = ['name', 'status', 'state', 'maritalStatus', 'creditScoreFICO', 'email', \n",
    "                 'numberOfChildren', \"predictedLifeEvent\"]\n",
    "    for l in data_list:\n",
    "        customer[l] = mongo_record[l] if l in mongo_record else 'N/A' \n",
    "\n",
    "    # Fields with money\n",
    "    data_list = ['investableAssetEstimate', 'zillowHomeValueEstimation', 'homeEquityEstimate']\n",
    "    for l in data_list:\n",
    "        customer[l] = \"$ {0:,d}\".format(int(mongo_record[l])) if l in mongo_record else 'N/A'\n",
    "    \n",
    "    \n",
    "    # \"Profitability\" time series\n",
    "    timeseries = mongo_record['monthlyProfitabilities']\n",
    "\n",
    "    month_list = []\n",
    "    this_year_values = []\n",
    "    last_year_values = []\n",
    "\n",
    "    if len(timeseries) > 1:\n",
    "        timeseries = sorted(timeseries, key=lambda r: r['date'], reverse = True) # sort by year+month\n",
    "        timeseries = timeseries[:20] # keep no more than 20 last months\n",
    "\n",
    "        # make our data look like they are fresh\n",
    "        data_date = date(int(timeseries[0]['date'][:4]), int(timeseries[0]['date'][5:]), 1)\n",
    "        our_date = date.today()-relativedelta(months=1)\n",
    "\n",
    "        for i in range(0, 4):\n",
    "            month_list.append(our_date.strftime(\"%b\"))\n",
    "\n",
    "            look_for_date = \"{0}-{1:02d}\".format(data_date.year, data_date.month)\n",
    "            profitability = next( (r['profitability'] \n",
    "                                   for r in timeseries \n",
    "                                   if r['date']==look_for_date)\n",
    "                                 , None)\n",
    "            this_year_values.append(profitability)\n",
    "\n",
    "            look_for_date = \"{0}-{1:02d}\".format(data_date.year-1, data_date.month)\n",
    "            profitability = next( (r['profitability'] \n",
    "                                   for r in timeseries \n",
    "                                   if r['date']==look_for_date)\n",
    "                                 , None)\n",
    "            last_year_values.append(profitability)\n",
    "\n",
    "            our_date -= relativedelta(months=1)\n",
    "            data_date -= relativedelta(months=1)\n",
    "\n",
    "        month_list.reverse()\n",
    "        this_year_values.reverse()\n",
    "        last_year_values.reverse()\n",
    "    \n",
    "    customer['profitability_months'] = month_list\n",
    "    customer['profitability_this_year'] = this_year_values\n",
    "    customer['profitability_last_year'] = last_year_values\n",
    "\n",
    "    # \"Products\" time series\n",
    "    timeseries = mongo_record['products']\n",
    "    month_list = []\n",
    "    investment_values = []\n",
    "    savings_values = []\n",
    "    checking_values = []\n",
    "\n",
    "    if len(timeseries) > 1: \n",
    "        timeseries = sorted(timeseries, key=lambda r: r['yearAndMonth'], reverse = True)\n",
    "        timeseries = timeseries[:60] # keep no more than 60 last values (up to 20 months)\n",
    "\n",
    "        # make our data look like they are fresh\n",
    "        data_date = date(int(timeseries[0]['yearAndMonth'][:4]), # year\n",
    "                         int(timeseries[0]['yearAndMonth'][5:]), # month\n",
    "                         1)                                      # day\n",
    "        our_date = date.today()-relativedelta(months=1)\n",
    "\n",
    "        for i in range(0, 4):\n",
    "            month_list.append(our_date.strftime(\"%b\"))\n",
    "\n",
    "            look_for_date = \"{0}-{1:02d}\".format(data_date.year, data_date.month)\n",
    "            investment = next( (r['monthlyAverage'] \n",
    "                                for r in timeseries \n",
    "                                if r['yearAndMonth']==look_for_date and r['product']==\"Investment\")\n",
    "                              , None)\n",
    "            investment_values.append(investment)\n",
    "            savings = next( (r['monthlyAverage'] \n",
    "                                for r in timeseries \n",
    "                                if r['yearAndMonth']==look_for_date and r['product']==\"Savings\")\n",
    "                              , None)\n",
    "            savings_values.append(savings)\n",
    "            checking = next( (r['monthlyAverage'] \n",
    "                                for r in timeseries \n",
    "                                if r['yearAndMonth']==look_for_date and r['product']==\"Checking\")\n",
    "                              , None)\n",
    "            checking_values.append(checking)\n",
    "\n",
    "            our_date -= relativedelta(months=1)\n",
    "            data_date -= relativedelta(months=1)\n",
    "\n",
    "        month_list.reverse()\n",
    "        investment_values.reverse()\n",
    "        savings_values.reverse()\n",
    "        checking_values.reverse()\n",
    "\n",
    "    customer['products_months'] = month_list\n",
    "    customer['investment_values'] = investment_values\n",
    "    customer['savings_values'] = savings_values\n",
    "    customer['checking_values'] = checking_values\n",
    "\n",
    "    # \"Surveys\" time series\n",
    "    timeseries = mongo_record['surveys']\n",
    "    surveys_data = []\n",
    "    \n",
    "    if len(timeseries) > 0: \n",
    "        timeseries = sorted(timeseries, key=lambda r: r['surveyDate'], reverse = True)\n",
    "        timeseries = timeseries[:20] # keep no more than 20 last values\n",
    "\n",
    "        # make our data look like they are fresh\n",
    "        data_date = datetime.strptime(timeseries[0]['surveyDate'], \"%Y-%m-%d\").date()\n",
    "        random.seed(userid)\n",
    "        last_survey = random.randint(0,1) # how long ago was last survey\n",
    "        date_from = date(data_date.year, data_date.month, 1) + relativedelta(months=-3+last_survey) \n",
    "        date_to = date(data_date.year, data_date.month, 1) + relativedelta(months=1+last_survey) \n",
    "        our_date = date.today()-relativedelta(months=1, day=15)\n",
    "        time_shift = our_date - date_to \n",
    "        \n",
    "        for r in timeseries:\n",
    "            r_date = datetime.strptime(r['surveyDate'], \"%Y-%m-%d\").date()\n",
    "            if r_date > date_from and r_date < date_to:\n",
    "                new_date = r_date + time_shift\n",
    "                utc_date = int( time.mktime((new_date.year, new_date.month, new_date.day, 0,0,0,0,0,0)) * 1000 )\n",
    "                surveys_data.append([utc_date, r['surveyResult']])\n",
    "\n",
    "        #date_from = date_from + time_shift\n",
    "        #date_to = date_to + time_shift\n",
    "\n",
    "    date_from = date.today()-relativedelta(months=5, day=17)\n",
    "    date_to = date.today()-relativedelta(months=1, day=17)\n",
    "\n",
    "    customer['surveys_data'] = surveys_data\n",
    "    customer['surveys_date_from'] = int( time.mktime((date_from.year, date_from.month, date_from.day, 0,0,0,0,0,0)) * 1000 )\n",
    "    customer['surveys_date_to'] = int( time.mktime((date_to.year, date_to.month, date_to.day, 0,0,0,0,0,0)) * 1000 )\n",
    "\n",
    "    nbo_offers = mongo_record['nextBestOffers']\n",
    "    if len(nbo_offers) > 0: \n",
    "        nbo_offers = sorted(nbo_offers, key=lambda r: r['nboProbability'], reverse = True)\n",
    "        i = 1\n",
    "        for r in nbo_offers:\n",
    "            r['nboProbability'] = \"{0:.1f}%\".format(r['nboProbability']*100.0)\n",
    "            r['index'] = i\n",
    "            i += 1\n",
    "    customer['nbo_offers'] = nbo_offers\n",
    "    \n",
    "    customer['suboffers'] = suboffers\n",
    "\n",
    "    # List of last 4 months\n",
    "    customer['last4months'] = [(date.today()-relativedelta(months=a)).strftime('%b') for a in range(4,0,-1)]\n",
    "\n",
    "    try:\n",
    "        return render_template(\"nbo.html\", customer=customer)\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/kli/<userid>')\n",
    "def klipage(userid):\n",
    "    try:\n",
    "        return render_template(\"kli.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/analysis')\n",
    "def analysispage():\n",
    "    try:\n",
    "        return render_template(\"analysis.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/loopfeedback')\n",
    "def homepage():\n",
    "    try:\n",
    "        return render_template(\"home.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/')\n",
    "def welcomepage():\n",
    "    try:\n",
    "        return render_template(\"welcome.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from gevent.pool import Pool\n",
    "    from gevent import pywsgi\n",
    "    from geventwebsocket.handler import WebSocketHandler\n",
    "    server = pywsgi.WSGIServer(('', 8090), app, handler_class=WebSocketHandler, spawn=Pool())\n",
    "    server.serve_forever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
