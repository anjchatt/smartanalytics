{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import os\n",
    "import psycopg2\n",
    "import psycopg2.extensions\n",
    "\n",
    "from py2neo import Graph\n",
    "from twitter import Twitter, OAuth\n",
    "\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import random\n",
    "import csv\n",
    "from string import strip\n",
    "from math import floor, ceil\n",
    "\n",
    "# for sending offers via gmail\n",
    "import base64\n",
    "from email.mime.image import MIMEImage\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import smtplib\n",
    "\n",
    "# Natural language query parser\n",
    "import parse_query\n",
    "    \n",
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.customer_database\n",
    "\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "graph = Graph(config[\"neo4j\"])\n",
    "\n",
    "if 'http_proxy' in os.environ:\n",
    "    if 'https_proxy' not in os.environ:\n",
    "        os.environ[\"https_proxy\"] = os.environ[\"http_proxy\"]\n",
    "\n",
    "twitter = Twitter(auth=OAuth(config['token'], config['token_secret'], config['consumer_key'], config['consumer_secret']))\n",
    "\n",
    "field_names =  ['user_screen_name', 'user_name', 'id_str', 'created_at', \n",
    "                'sentiment', 'categories', 'text', 'klout_score', 'segment_id',\n",
    "               'got_reply', 'followers_count', 'is_agent_reply']\n",
    "\n",
    "segment_dict = {0:'-', 1:'Mass Market', 2:'Young Professional',\n",
    "                3:'Mass Afluent', 4:'Affluent', 5:'High Net Worth'}\n",
    "\n",
    "#suboffers = {} # offers dictionary\n",
    "#with open('offers_suboffers.csv', 'rb') as csvfile:\n",
    "#    file_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "#    for row in file_reader:\n",
    "#        if len(row) >= 3:          # check if we have at least 3 field\n",
    "#            key = strip(row[0])\n",
    "#            value = strip(row[1])\n",
    "#            imageId = strip(row[2])\n",
    "#            text = strip(row[3]) if len(row) >= 4 else \"\"\n",
    "#            if key in suboffers:\n",
    "#                suboffers[key].append({\"name\": value, \"image\": imageId, \"text\":text})\n",
    "#            else:\n",
    "#                suboffers[key] = [{\"name\": value, \"image\": imageId, \"text\":text}]\n",
    "\n",
    "def get_priority_key(tw_l):\n",
    "    for tw in tw_l:\n",
    "        if not tw['is_agent_reply']:\n",
    "            p = (tw['segment_id']/2 - tw['sentiment'] - tw['got_reply']*5 +\n",
    "                tw['followers_count']/1000 + int('fraud' in tw['categories']))\n",
    "            return -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  [('state', 23, 25), ('debt_funds', 9, 18)]\n",
      "5\n",
      "3\n",
      "SELECT\n",
      "[]\n",
      "WHERE\n",
      "[\"debt_funds in ('yes')\", \"state in ('New York')\"]\n",
      "GROUP BY\n",
      "[]\n",
      "LIMIT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, jsonify, request, send_file\n",
    "from flask_sockets import Sockets\n",
    "from json import dumps\n",
    "import gevent\n",
    "import time\n",
    "import StringIO\n",
    "\n",
    "app = Flask(__name__)\n",
    "sockets = Sockets(app)\n",
    "\n",
    "@app.route('/voc')\n",
    "def voc():\n",
    "    try:\n",
    "        return render_template(\"voc.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/720page/<name>')\n",
    "def foo_720page(name):\n",
    "    try:\n",
    "        return render_template(\"720page.html\", name=name)\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/720view/<name>')\n",
    "def foo_720view(name):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        customer = db.customers.find_one(\n",
    "            { 'name' :  name},\n",
    "            {'_id':  0,\n",
    "             'photo':1,\n",
    "             'name':1,\n",
    "             'phone':1,\n",
    "             'status':1}\n",
    "        )\n",
    "        \"\"\"\n",
    "        if name.lower()[:8]=='kathleen':\n",
    "            customer = {\n",
    "                'name': 'Kathleen Fanning',\n",
    "                'photo': '/static/images/photo_female.png',\n",
    "                'phone': '212-2222-245',\n",
    "                'status': 'Married'\n",
    "            }\n",
    "        else:\n",
    "            customer = {\n",
    "                'name': 'John Smith',\n",
    "                'photo': '/static/images/photo_male.png',\n",
    "                'phone': '212-2222-245',\n",
    "                'status': 'Married'\n",
    "            }\n",
    "        return render_template(\"720view.html\", customer=customer)\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "@sockets.route('/twitter_stream')\n",
    "def twitter_stream(ws):\n",
    "    j = 0\n",
    "    \n",
    "    conn = psycopg2.connect(database=\"twitter\", user=\"postgres\")\n",
    "    conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    curs = conn.cursor()\n",
    "    curs.execute(\"LISTEN new_tweet;\")\n",
    "    \n",
    "    while not ws.closed:\n",
    "        conn.poll()\n",
    "        t0 = time.time()\n",
    "        if conn.notifies:\n",
    "            notify = conn.notifies.pop(0)\n",
    "            \n",
    "            ws.send(\"{notification:\\'new tweet\\'}\")\n",
    "            j+=1\n",
    "        if time.time() - t0 > 3600:\n",
    "            print(\"1hr timeout\")\n",
    "            break\n",
    "        else:\n",
    "            gevent.sleep()\n",
    "    conn.close()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@app.route('/reply', methods=['POST'])\n",
    "def profile():\n",
    "    data = eval(request.form['py_data'])\n",
    "    status = request.form['status']\n",
    "    status_id = request.form['status_id']\n",
    "    \n",
    "    sentiment = data['sentiment']\n",
    "    user_name = data['user_name']\n",
    "    \n",
    "    tweet = twitter.statuses.update(status=status, in_reply_to_status_id=status_id)\n",
    "    \n",
    "    conn = psycopg2.connect(database=config[\"twitter_db\"][\"database\"], user=config[\"twitter_db\"][\"user\"])\n",
    "    conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    curs = conn.cursor()\n",
    "    \n",
    "    curs.execute(\"\"\"update customers\n",
    "                set got_reply=1\n",
    "                where user_screen_name=%s\"\"\",(tweet['in_reply_to_screen_name'],))\n",
    "    \n",
    "    curs.execute(\"\"\"insert into stream (id_str, text, created_at, in_reply_to_status_id,\n",
    "                in_reply_to_screen_name, user_screen_name, user_name, sentiment, is_agent_reply)\n",
    "                values (%s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\",\n",
    "                 (tweet['id_str'],\n",
    "                  tweet['text'],\n",
    "                  tweet['created_at'],\n",
    "                  tweet['in_reply_to_status_id'],\n",
    "                  tweet['in_reply_to_screen_name'],\n",
    "                  tweet['in_reply_to_screen_name'],\n",
    "                  user_name,\n",
    "                  sentiment,\n",
    "                  1,))\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    return 'good'\n",
    "\n",
    "@app.route('/send_offer', methods=['POST'])\n",
    "def send_offer():\n",
    "    recipient_name = request.form['recipient-name']\n",
    "    message_text = request.form['message-text']\n",
    "    offer_image_id = request.form['offer-image-id']\n",
    "\n",
    "    gmail_user = config[\"gmail_user\"]\n",
    "    gmail_pass = config[\"gmail_pass\"]\n",
    "    gmail_recipient = config[\"gmail_recipient\"]\n",
    "    TO = gmail_recipient if type(gmail_recipient) is list else [gmail_recipient]\n",
    "\n",
    "    msg = MIMEMultipart('alternative')\n",
    "    msg['Subject'] = \"Special offer from CapBank\"\n",
    "    msg['From'] = \"VIP clients support team\"\n",
    "    msg['To'] = recipient_name\n",
    "    \n",
    "    fp = open(\"static/images/email/email_tmp\"+offer_image_id+\".png\", 'rb')\n",
    "    img_b64 = base64.b64encode(fp.read()).decode('ascii')\n",
    "    fp.close()\n",
    "\n",
    "    html = \"\"\"<html>\n",
    "      <head></head>\n",
    "      <body>\n",
    "        <img src=\"data:image/png;base64,{0}\" /><br />\n",
    "        <pre>{1}</pre>\n",
    "      </body>\n",
    "    </html>\"\"\".format(img_b64, message_text)\n",
    "    \n",
    "    # Record the MIME types of both parts - text/plain and text/html.\n",
    "    part1 = MIMEText(text, 'plain')\n",
    "    part2 = MIMEText(html, 'html')    \n",
    "    \n",
    "    # Attach parts into message container.\n",
    "    # According to RFC 2046, the last part of a multipart message, in this case\n",
    "    # the HTML message, is best and preferred.\n",
    "    msg.attach(part1)\n",
    "    msg.attach(part2)\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "        server.ehlo()\n",
    "        server.starttls()\n",
    "        server.login(gmail_user, gmail_pass)\n",
    "        server.sendmail(gmail_user, gmail_recipient, msg.as_string())\n",
    "        server.close()\n",
    "        res = '<h4 class=\"bg-success\">E-mail sent successfully</h4>'\n",
    "    except:\n",
    "        res = '<h4 class=\"bg-danger\">E-mail sending failed</h4>'\n",
    "\n",
    "    return res\n",
    "\n",
    "@app.route('/data', methods=['GET'])\n",
    "def provide_data():\n",
    "    conn = psycopg2.connect(database=\"twitter\", user=\"postgres\")\n",
    "    conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    curs.execute(\"\"\"select stream.user_screen_name, stream.user_name, stream.id_str,\n",
    "                stream.created_at, stream.sentiment, stream.categories, stream.text,\n",
    "                customers.klout_score, customers.segment, customers.got_reply, \n",
    "                customers.followers_count, stream.is_agent_reply\n",
    "                from stream\n",
    "                INNER JOIN customers\n",
    "                ON customers.user_screen_name=stream.user_screen_name\n",
    "                where stream.user_screen_name in \n",
    "                (\n",
    "                    select tmp.user_screen_name\n",
    "                    from (\n",
    "                        select user_screen_name as user_screen_name, max(id_str) as max_id_str\n",
    "                        from stream\n",
    "                        group by user_screen_name\n",
    "                    ) tmp\n",
    "                    left join (\n",
    "                        select user_screen_name, got_reply\n",
    "                        from customers\n",
    "                    ) tmp2\n",
    "                    on tmp.user_screen_name=tmp2.user_screen_name\n",
    "                    order by tmp2.got_reply asc, tmp.max_id_str desc limit 10\n",
    "                )\n",
    "                order by customers.got_reply desc, stream.id_str desc\n",
    "                ;\"\"\")\n",
    "    rec = curs.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    user_blocks = {v[0]:[] for v in rec}\n",
    "    for row in rec:\n",
    "        if len(user_blocks[row[0]]) < 10:\n",
    "            user_blocks[row[0]] += [{k:v for k,v in zip(field_names, row)}]\n",
    "            user_blocks[row[0]][-1]['id_str'] = str(user_blocks[row[0]][-1]['id_str'])\n",
    "            user_blocks[row[0]][-1]['segment'] = segment_dict[user_blocks[row[0]][-1]['segment_id']]\n",
    "    \n",
    "    user_blocks_v = user_blocks.values()\n",
    "    user_blocks_v_s = sorted(user_blocks_v, key=get_priority_key)\n",
    "    \n",
    "    return jsonify({'data':user_blocks_v_s})\n",
    "\n",
    "@app.route('/graph')\n",
    "def get_graph():\n",
    "    results = graph.cypher.execute(\n",
    "        \"match (p)-[r]->(p1) \"\n",
    "        \"where ((r.favorited>0 and r.retweeted>0) or (r.favorited>0 and r.replied>0) or (r.retweeted>0 and r.replied>0) \"\n",
    "        \"or (r.favorited>1) or (r.retweeted>1) or (r.replied>1)) and (not p=p1) \"\n",
    "        \"return p.screen_name, sum(r.favorited)+sum(r.retweeted)+sum(r.replied) as cardinality, p1.screen_name \"\n",
    "        \"limit 500;\")\n",
    "    nodes = []\n",
    "    rels = []\n",
    "\n",
    "    persons = {}\n",
    "    for person_b, cardinality, person_a in results:\n",
    "        if person_a in persons:\n",
    "            persons[person_a] += 1\n",
    "        else:\n",
    "            persons[person_a] = 1\n",
    "\n",
    "        if person_b not in persons:\n",
    "            persons[person_b] = 1\n",
    "\n",
    "    for i, v in persons.iteritems():\n",
    "        if i == u'gunjan_amit':\n",
    "            person = {\n",
    "                \"id\": persons.keys().index(i),\n",
    "                #\"caption\": i, \n",
    "                \"caption\": \"User\", \n",
    "                \"role\": \"center\"}\n",
    "        else:\n",
    "            person = {\n",
    "                \"id\": persons.keys().index(i),\n",
    "                \"caption\": i, \n",
    "                \"role\": \"customer\" if np.random.rand()>0.9 else \"user\",\n",
    "                }\n",
    "        nodes.append(person)\n",
    "\n",
    "    for person_b, cardinality, person_a in results:\n",
    "        person = {\"caption\": person_a, \"role\": \"customer\"}\n",
    "        source = persons.keys().index(person_a)\n",
    "        target = persons.keys().index(person_b)\n",
    "        rels.append({\"source\": source, \n",
    "            \"target\": target, \n",
    "            \"weight\": cardinality,\n",
    "            \"caption\": \"Value: {0}\".format(cardinality)\n",
    "            })\n",
    "\n",
    "    return jsonify({\"comment\": \"Tweeter users graph\", \"nodes\": nodes, \"edges\": rels})\n",
    "\n",
    "@app.route('/download/customers')\n",
    "def index():\n",
    "    strIO = StringIO.StringIO()\n",
    "    strIO.write('Customer Name, Status, Churn Rate\\n')\n",
    "#    for i in range(10):\n",
    "#        strIO.write(str(np.floor(10000*np.random.rand()))+',Credit Card\\n')\n",
    "    \n",
    "    cursor = db.customers.find().limit(103)\n",
    "    \n",
    "    for  c in cursor:\n",
    "        strIO.write(str(c['name'])+','+str(c['status'])+','+str(round(c['churn_rate']*100))+'%\\n')\n",
    "\n",
    "    strIO.seek(0)\n",
    "    return send_file(strIO,\n",
    "                     attachment_filename=\"Customers.csv\",\n",
    "                     as_attachment=True)\n",
    "\n",
    "@app.route('/map/get_customers/<state>')\n",
    "def map_get_customers(state):\n",
    "    try:\n",
    "        #cursor = db.customers.find({ 'state' : state })\n",
    "        cursor = db.customers.find(\n",
    "            { 'state' : state },\n",
    "            {'_id':  0,'customerId':1,'name':1,'churn_rate':1,'status':1}\n",
    "        )\n",
    "\n",
    "        status_weights = {'platinum':10,'gold':9,'silver':8,'bronze':7}\n",
    "\n",
    "        customers = []\n",
    "        for c in cursor:\n",
    "            c.update({'importance':status_weights[c['status'].lower()]*c['churn_rate']})\n",
    "            customers.append(c)\n",
    "\n",
    "        customers = sorted(customers, key=lambda x:-x['importance'])[:100]\n",
    "\n",
    "        customers = [[c['importance'],\n",
    "                      c['customerId'],\n",
    "                      c['name'],\n",
    "                      str(int(c['churn_rate']*100))+' %',\n",
    "                      c['status'],] \n",
    "                     for c in customers]\n",
    "        # Set selected user\n",
    "        customers[0][1] = 1\n",
    "        customers[0][2] = 'Kathleen Fanning'\n",
    "        return jsonify({'data':customers})\n",
    "    \n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/map/get_states_data')\n",
    "def map_states_data():\n",
    "    try:\n",
    "        with open('static/js/us-states.json') as f:\n",
    "            us_states_obj = json.load(f)\n",
    "\n",
    "        states_data = db.customers.aggregate([\n",
    "            {\"$group\": {\"_id\": \"$state\", \n",
    "                        'averageChurn': { '$avg': \"$churn_rate\" },\n",
    "                        \"count\": {\"$sum\": 1}}}])\n",
    "        states_data = {row['_id']:row for row in states_data}\n",
    "\n",
    "        states_data_objs = []\n",
    "\n",
    "        acc = 0\n",
    "        for i in range(len(us_states_obj['features'])):\n",
    "            state_name = us_states_obj['features'][i][u'properties']['name']\n",
    "            if state_name in states_data:\n",
    "                us_states_obj['features'][i]['properties'].update(states_data[state_name])\n",
    "                states_data_objs.append(us_states_obj['features'][i])\n",
    "\n",
    "        us_states_obj['features'] = states_data_objs\n",
    "\n",
    "        return jsonify(us_states_obj)\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/map')\n",
    "def mappage():\n",
    "    try:\n",
    "        return render_template(\"map.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/cluster')\n",
    "def clusterpage():\n",
    "    try:\n",
    "        return render_template(\"cluster.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/nbo/<int:userid>')\n",
    "def nbopage(userid):\n",
    "    try:\n",
    "        cursor = db.customers.find({ 'customerId' : userid })\n",
    "        if cursor.count() < 1:\n",
    "            return render_template(\"nbo_no_such_user.html\")\n",
    "        mongo_record = cursor.next()\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "    customer = {}\n",
    "    # General fields\n",
    "    data_list = ['name', 'status', 'state', 'maritalStatus', 'creditScoreFICO', 'email', \n",
    "                 'numberOfChildren', \"predictedLifeEvent\"]\n",
    "    for l in data_list:\n",
    "        customer[l] = mongo_record[l] if l in mongo_record else 'N/A' \n",
    "\n",
    "    # Fields with money\n",
    "    data_list = ['investableAssetEstimate', 'zillowHomeValueEstimation', 'homeEquityEstimate']\n",
    "    for l in data_list:\n",
    "        customer[l] = \"$ {0:,d}\".format(int(mongo_record[l])) if l in mongo_record else 'N/A'\n",
    "    \n",
    "    \n",
    "    # \"Profitability\" time series\n",
    "    timeseries = mongo_record['monthlyProfitabilities']\n",
    "\n",
    "    month_list = []\n",
    "    this_year_values = []\n",
    "    last_year_values = []\n",
    "\n",
    "    if len(timeseries) > 1:\n",
    "        timeseries = sorted(timeseries, key=lambda r: r['date'], reverse = True) # sort by year+month\n",
    "        timeseries = timeseries[:20] # keep no more than 20 last months\n",
    "\n",
    "        # make our data look like they are fresh\n",
    "        data_date = date(int(timeseries[0]['date'][:4]), int(timeseries[0]['date'][5:]), 1)\n",
    "        our_date = date.today()-relativedelta(months=1)\n",
    "\n",
    "        for i in range(0, 4):\n",
    "            month_list.append(our_date.strftime(\"%b\"))\n",
    "\n",
    "            look_for_date = \"{0}-{1:02d}\".format(data_date.year, data_date.month)\n",
    "            profitability = next( (r['profitability'] \n",
    "                                   for r in timeseries \n",
    "                                   if r['date']==look_for_date)\n",
    "                                 , None)\n",
    "            this_year_values.append(profitability)\n",
    "\n",
    "            look_for_date = \"{0}-{1:02d}\".format(data_date.year-1, data_date.month)\n",
    "            profitability = next( (r['profitability'] \n",
    "                                   for r in timeseries \n",
    "                                   if r['date']==look_for_date)\n",
    "                                 , None)\n",
    "            last_year_values.append(profitability)\n",
    "\n",
    "            our_date -= relativedelta(months=1)\n",
    "            data_date -= relativedelta(months=1)\n",
    "\n",
    "        month_list.reverse()\n",
    "        this_year_values.reverse()\n",
    "        last_year_values.reverse()\n",
    "    \n",
    "    customer['profitability_months'] = month_list\n",
    "    customer['profitability_this_year'] = this_year_values\n",
    "    customer['profitability_last_year'] = last_year_values\n",
    "\n",
    "    # \"Products\" time series\n",
    "    timeseries = mongo_record['products']\n",
    "    month_list = []\n",
    "    investment_values = []\n",
    "    savings_values = []\n",
    "    checking_values = []\n",
    "\n",
    "    if len(timeseries) > 1: \n",
    "        timeseries = sorted(timeseries, key=lambda r: r['yearAndMonth'], reverse = True)\n",
    "        timeseries = timeseries[:60] # keep no more than 60 last values (up to 20 months)\n",
    "\n",
    "        # make our data look like they are fresh\n",
    "        data_date = date(int(timeseries[0]['yearAndMonth'][:4]), # year\n",
    "                         int(timeseries[0]['yearAndMonth'][5:]), # month\n",
    "                         1)                                      # day\n",
    "        our_date = date.today()-relativedelta(months=1)\n",
    "\n",
    "        for i in range(0, 4):\n",
    "            month_list.append(our_date.strftime(\"%b\"))\n",
    "\n",
    "            look_for_date = \"{0}-{1:02d}\".format(data_date.year, data_date.month)\n",
    "            investment = next( (r['monthlyAverage'] \n",
    "                                for r in timeseries \n",
    "                                if r['yearAndMonth']==look_for_date and r['product']==\"Investment\")\n",
    "                              , None)\n",
    "            investment_values.append(investment)\n",
    "            savings = next( (r['monthlyAverage'] \n",
    "                                for r in timeseries \n",
    "                                if r['yearAndMonth']==look_for_date and r['product']==\"Savings\")\n",
    "                              , None)\n",
    "            savings_values.append(savings)\n",
    "            checking = next( (r['monthlyAverage'] \n",
    "                                for r in timeseries \n",
    "                                if r['yearAndMonth']==look_for_date and r['product']==\"Checking\")\n",
    "                              , None)\n",
    "            checking_values.append(checking)\n",
    "\n",
    "            our_date -= relativedelta(months=1)\n",
    "            data_date -= relativedelta(months=1)\n",
    "\n",
    "        month_list.reverse()\n",
    "        investment_values.reverse()\n",
    "        savings_values.reverse()\n",
    "        checking_values.reverse()\n",
    "\n",
    "    customer['products_months'] = month_list\n",
    "    customer['investment_values'] = investment_values\n",
    "    customer['savings_values'] = savings_values\n",
    "    customer['checking_values'] = checking_values\n",
    "\n",
    "    # \"Surveys\" time series\n",
    "    timeseries = mongo_record['surveys']\n",
    "    surveys_data = []\n",
    "    last4years = []\n",
    "    \n",
    "    if len(timeseries) > 0: \n",
    "        years_show = 4\n",
    "        timeseries = sorted(timeseries, key=lambda r: r['surveyDate'], reverse = True)\n",
    "        timeseries = timeseries[:years_show] # keep no more than 4 last values\n",
    "\n",
    "        # make our data look like they are for last 4 years\n",
    "        date_to = date.today()\n",
    "        for r in timeseries:\n",
    "            date_to = date_to-relativedelta(years=1)\n",
    "            surveys_data.append(r['surveyResult'])\n",
    "            last4years.append(date_to.year)\n",
    "        # fill empty values with None\n",
    "        for i in range(len(timeseries), years_show):\n",
    "            date_to = date_to-relativedelta(years=1)\n",
    "            surveys_data.append(None)\n",
    "            last4years.append(date_to.year)\n",
    "\n",
    "    surveys_data.reverse()\n",
    "    last4years.reverse()\n",
    "    customer['surveys_data'] = surveys_data\n",
    "    customer['last4years'] = last4years\n",
    "\n",
    "    nbo_offers = mongo_record['nextBestOffers']\n",
    "    if len(nbo_offers) > 0: \n",
    "        nbo_offers = sorted(nbo_offers, key=lambda r: r['nboProbability'], reverse = True)\n",
    "        i = 1\n",
    "        for r in nbo_offers:\n",
    "            r['nboProbability'] = \"{0:.1f}%\".format(r['nboProbability']*100.0)\n",
    "            r['index'] = i\n",
    "            i += 1\n",
    "    customer['nbo_offers'] = nbo_offers\n",
    "    \n",
    "#    customer['suboffers'] = suboffers\n",
    "\n",
    "    # List of last 4 months\n",
    "    customer['last4months'] = [(date.today()-relativedelta(months=a)).strftime('%b') for a in range(4,0,-1)]\n",
    "\n",
    "    try:\n",
    "        return render_template(\"nbo.html\", customer=customer)\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/loopfeedback')\n",
    "def loopfeedbackpage():\n",
    "    try:\n",
    "        return render_template(\"loopfeedback.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/loyalty')\n",
    "def loyaltypage():\n",
    "    try:\n",
    "        return render_template(\"loyalty.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/wm', methods=['GET'])\n",
    "def wmpage():\n",
    "    try:\n",
    "        name = ''\n",
    "        status = 'ok'\n",
    "        if 'name' in request.args:\n",
    "            name = request.args['name']\n",
    "            if name.lower() == 'deborah winshel':\n",
    "                name = 'Deborah Winshel'\n",
    "            else:\n",
    "                name = str(name)\n",
    "                status = 'Not found'\n",
    "        else:\n",
    "            status = 'Search'\n",
    "        \n",
    "        return render_template(\"wm.html\", name=name, status=status)\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/pci')\n",
    "def pcipage():\n",
    "    try:\n",
    "        return render_template(\"pci.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "@app.route('/send_pci_query', methods=['POST'])\n",
    "def send_pci_query():\n",
    "    query_text = request.form['query']\n",
    "\n",
    "    def err_msg(text = 'Cannot understand you query, please try to paraphrase it'):\n",
    "        return jsonify({'error': 'yes', 'error_text': text,  'query': query_text})\n",
    "\n",
    "    def one_hot_dataframe(data, cols, replace=False):\n",
    "        vec = DictVectorizer()\n",
    "        mkdict = lambda row: dict((col, row[col]) for col in cols)\n",
    "        vecData = pandas.DataFrame(vec.fit_transform(data[cols].apply(mkdict, axis=1)).toarray())\n",
    "        vecData.columns = vec.get_feature_names()\n",
    "        vecData.index = data.index\n",
    "        if replace is True:\n",
    "            data = data.drop(cols, axis=1)\n",
    "            data = data.join(vecData)\n",
    "        return (data, vecData, vec)\n",
    "\n",
    "\n",
    "    query = parse_query.parse_it(query_text)\n",
    "\n",
    "    # check if it is predictive query\n",
    "    if query and 'parameters' in query and 'mode' in query['parameters'] and query['parameters']['mode'] == 'model':\n",
    "        #search for target variable\n",
    "        target = None\n",
    "        target_variables = ['debt_funds', 'credit_card']\n",
    "        new_filter = []\n",
    "        for sel in query['sql_filter']:\n",
    "            for var in target_variables:\n",
    "                if var in sel:\n",
    "                    target = var\n",
    "                    break \n",
    "            else: # do not keep target variable in filter list (for-else python structure)\n",
    "                new_filter.append(sel) \n",
    "\n",
    "        if target == None: \n",
    "            return err_msg('Cannot understand you query, please try to paraphrase it') # couldn't find target \n",
    "        \n",
    "        parameters = set([\"age\" , \"job\" , \"marital\" , \"education\" , \"debt_funds\" , \"credit_card\" , \n",
    "                          \"income\" , \"state\" , \"gender\"])\n",
    "        parameters = list(parameters - set([target]))\n",
    "        \n",
    "        sql_query = \"SELECT \" + target + \", \" + \", \".join(parameters) + \" FROM pci_customers\"\n",
    "        if new_filter: sql_query += \" WHERE \" + \" AND \".join(new_filter)\n",
    "\n",
    "        conn = psycopg2.connect(database=\"pci\", user=\"postgres\")\n",
    "\n",
    "        try:\n",
    "            full_df = pandas.read_sql(sql_query, conn)\n",
    "        except Exception, e:\n",
    "            return err_msg(str(e))\n",
    "\n",
    "        conn.close()\n",
    "        \n",
    "        y_name = full_df.keys()[0]\n",
    "        x = full_df.drop(y_name, axis=1)\n",
    "        y = full_df[y_name].copy()\n",
    "        \n",
    "        y.replace(to_replace=['no', 'yes'], value=[0, 1] , inplace = True)\n",
    "        x.replace(to_replace=['no', 'yes'], value=[0, 1] , inplace = True)\n",
    "        x.replace(to_replace=['F', 'M'], value=[0, 1] , inplace = True)\n",
    "        \n",
    "        x2, _, _ = one_hot_dataframe(x, ['state', 'marital', 'job', 'education'], replace=True)\n",
    "        \n",
    "        dtc = DecisionTreeClassifier(random_state=1120)\n",
    "        dtc.fit(x2, y)\n",
    "        feature_w = dict() # Feature weights\n",
    "        for i in range(len(x2.columns)):\n",
    "            if '=' in x2.columns[i]:\n",
    "                name = x2.columns[i].split('=')[0] # One-hot feature names are like 'job=student'\n",
    "            else:\n",
    "                name = x2.columns[i]\n",
    "            if name in feature_w:\n",
    "                feature_w[name] += dtc.feature_importances_[i]\n",
    "            else:\n",
    "                feature_w[name] = dtc.feature_importances_[i]\n",
    "        \n",
    "        # Convert to descending list\n",
    "        #feature_w = sorted(feature_w.items(), key = lambda x: -x[1])\n",
    "        \n",
    "        # Convert to list (when unordered it looks better on spider-web type chart)\n",
    "        feature_w = feature_w.items()\n",
    "        \n",
    "        # fill with default data\n",
    "        charts = {'age': 0,\n",
    "                 'gender': 0,\n",
    "                 'state': 0,\n",
    "                 'credit_card': 0,\n",
    "                 'debt_funds': 0,\n",
    "                 'job': 0,\n",
    "                 'income': 0,\n",
    "                 'education': 0,\n",
    "                 'marital': 0}\n",
    "\n",
    "\n",
    "\n",
    "        # Predictors is list of tuples (<parameter>, <Parameter name>)\n",
    "        predictors = [('age', 'Age'), ('income', 'Income'), ('job', 'Job type'), ('education', 'Education level'),\n",
    "                     ('marital', 'Marital status'), ('gender', 'Gender')]\n",
    "        # Collect data\n",
    "        for predictor, pr_name in predictors:\n",
    "            if predictor == 'age':\n",
    "                series = full_df.groupby([ y_name, np.floor(full_df[predictor]/10)*10 ]).count()['age']\n",
    "            elif predictor == 'income':\n",
    "                series = full_df.groupby([ y_name, np.floor(full_df[predictor]/1000)*1000 ]).count()['age']\n",
    "            else:\n",
    "                series = full_df.groupby([y_name, predictor]).count()['age']\n",
    "            \n",
    "            res_series = []\n",
    "            for i in series.index.levels[0]:\n",
    "                series_data = []\n",
    "                for k in series.index.levels[1]:\n",
    "                    if (i, k) in series:\n",
    "                        series_data.append( series[(i, k)] )\n",
    "                    else:\n",
    "                        series_data.append( None )\n",
    "                series_name = \"Positive\" if i == \"yes\" else \"Negative\"\n",
    "                res_series.append({\"name\": series_name, \"data\": series_data})\n",
    "            res_series.reverse()\n",
    "\n",
    "\n",
    "            params = {'name': pr_name}\n",
    "            params['categories'] = list(series.index.levels[1])\n",
    "            params['series'] = res_series\n",
    "\n",
    "            #save predictor to charts dictionary\n",
    "            charts[predictor] = params\n",
    "            \n",
    "        # Collect States data \n",
    "        series = full_df.groupby([y_name, 'state']).count()['age']\n",
    "        states = dict()\n",
    "        # We'll calculate max and min values for map coloring\n",
    "        max_val = 0\n",
    "        min_val = 100\n",
    "        for k in series.index.levels[1]:\n",
    "            if ('yes', k) in series: yes = series[('yes', k)]\n",
    "            else: yes = 0\n",
    "            if ('no', k) in series: no = series[('no', k)]\n",
    "            else: no = 0\n",
    "            if yes+no > 0:\n",
    "                val = floor(yes*100/(yes+no))\n",
    "                states[k] = {\"value\": val}\n",
    "                if val > max_val: max_val = val\n",
    "                if val < min_val: min_val = val\n",
    "            else: states[k] = {\"value\": None}        \n",
    "        params = {'name': 'State distribution'}\n",
    "        params['states'] = states\n",
    "        params['minValue'] = min_val\n",
    "        params['maxValue'] = max_val\n",
    "\n",
    "        #save predictor to charts dictionary\n",
    "        charts['state'] = params\n",
    "        \n",
    "        res = jsonify({ 'type': 'model', 'title': '', 'query': query_text, 'features': feature_w, 'charts': charts })\n",
    "        \n",
    "    else: # not a model query\n",
    "        if not query or (not query['sql_select']): \n",
    "            return err_msg()\n",
    "\n",
    "        sql_query = \"SELECT \" + \",\".join(query['sql_select']) + \" FROM pci_customers \"\n",
    "        if query['sql_filter']:\n",
    "            sql_query += \" WHERE \" + \" AND \".join(query['sql_filter'])\n",
    "        if query['sql_group_by']:\n",
    "            sql_query += \" GROUP BY \" + \", \".join(query['sql_group_by']) \n",
    "            sql_query += \" ORDER BY \" + \", \".join(query['sql_group_by'])\n",
    "        if query['sql_limit']:\n",
    "            sql_query += \" LIMIT \" + query['sql_limit']\n",
    "\n",
    "        chartTitle = \"Shown: '\" + query['target']\n",
    "        if query['group_by'] and query['group_by'] != query['target']:\n",
    "            chartTitle += \" by \" + query['group_by'] + \"'\"\n",
    "        if query['sql_filter']:\n",
    "            chartTitle += \" Filter: \" + \" AND \".join(query['sql_filter']) + \"'\"\n",
    "\n",
    "        if query['group_by'] == \"state\":\n",
    "            chartType = \"map\"\n",
    "        elif query['group_by'] == \"gender\":\n",
    "            chartType = \"pie\"\n",
    "            category_ind = query['sql_select'].index('gender')\n",
    "        elif query['group_by'] == \"job\":\n",
    "            chartType = \"pie\"\n",
    "            category_ind = query['sql_select'].index('job')\n",
    "        elif query['group_by'] == \"education\":\n",
    "            chartType = \"pie\"\n",
    "            category_ind = query['sql_select'].index('education')\n",
    "        elif query['group_by']:\n",
    "            chartType = \"linear\"\n",
    "        else:\n",
    "            chartType = \"table\"\n",
    "  \n",
    "        xAxis = []\n",
    "        yAxis = []\n",
    "        print sql_query # debug\n",
    "\n",
    "        conn = psycopg2.connect(database=\"pci\", user=\"postgres\")\n",
    "        conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "        curs = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            curs.execute(sql_query)\n",
    "        except Exception, e:\n",
    "            return err_msg()\n",
    "\n",
    "        rec = curs.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        res = err_msg()\n",
    "\n",
    "        if chartType == 'pie':\n",
    "            value_ind = 1 if category_ind == 0 else 0\n",
    "            for row in rec:\n",
    "                yAxis.append(round(row[value_ind]))\n",
    "                xAxis.append(row[category_ind])\n",
    "            res = jsonify({'type': chartType, 'title': chartTitle, 'xAxis': xAxis, 'yAxis': yAxis, 'query': query_text})\n",
    "\n",
    "        if chartType == 'linear': \n",
    "            for row in rec:\n",
    "                yAxis.append(round(row[0]))\n",
    "                xAxis.append(round(row[1]))\n",
    "            res = jsonify({'type': chartType, 'title': chartTitle, 'xAxis': xAxis, 'yAxis': yAxis,\n",
    "                          'query': query_text, 'xTitle': query['group_by'], 'yTitle': query['target'] })\n",
    "\n",
    "        if chartType == 'map':\n",
    "            states = dict()\n",
    "\n",
    "            # one of SELECT parameters will be state and other - values\n",
    "            state_ind = 0 if query['sql_select'][0] == 'state' else 1\n",
    "            count_ind = 1 if state_ind == 0 else 0\n",
    "\n",
    "            if len(rec)>0: max_val = rec[0][count_ind] # We'll calculate max and min values for map coloring\n",
    "            else: max_val = 0\n",
    "            min_val = max_val\n",
    "            for row in rec:\n",
    "                states[row[state_ind]] = {\"value\": round(row[count_ind])}\n",
    "                if round(row[count_ind]) > max_val: max_val = round(row[count_ind])\n",
    "                if round(row[count_ind]) < min_val: min_val = round(row[count_ind])\n",
    "            res = jsonify({'type': chartType, 'title': chartTitle, \n",
    "                           'xAxis': [], 'yAxis': [], \n",
    "                           'states': states, 'query': query_text,\n",
    "                           \"maxValue\": ceil(max_val), \"minValue\": floor(min_val)})\n",
    "\n",
    "    return res;\n",
    "\n",
    "@app.route('/')\n",
    "def welcomepage():\n",
    "    try:\n",
    "        return render_template(\"welcome.html\")\n",
    "    except Exception, e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from gevent.pool import Pool\n",
    "    from gevent import pywsgi\n",
    "    from geventwebsocket.handler import WebSocketHandler\n",
    "    server = pywsgi.WSGIServer(('', 8091), app, handler_class=WebSocketHandler, spawn=Pool())\n",
    "    server.serve_forever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
